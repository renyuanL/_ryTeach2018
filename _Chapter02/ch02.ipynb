{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2017\n",
    "\n",
    "Code Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition\n",
    "\n",
    "Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Machine Learning - Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 - Training Machine Learning Algorithms for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext watermark\n",
    "#%watermark -a \"Sebastian Raschka\" -u -d -p numpy,pandas,matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*The use of `watermark` is optional. You can install this IPython extension via \"`pip install watermark`\". For more information, please see: https://github.com/rasbt/watermark.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Artificial neurons – a brief glimpse into the early history of machine learning](#Artificial-neurons-a-brief-glimpse-into-the-early-history-of-machine-learning)\n",
    "    - [The formal definition of an artificial neuron](#The-formal-definition-of-an-artificial-neuron)\n",
    "    - [The perceptron learning rule](#The-perceptron-learning-rule)\n",
    "- [Implementing a perceptron learning algorithm in Python](#Implementing-a-perceptron-learning-algorithm-in-Python)\n",
    "    - [An object-oriented perceptron API](#An-object-oriented-perceptron-API)\n",
    "    - [Training a perceptron model on the Iris dataset](#Training-a-perceptron-model-on-the-Iris-dataset)\n",
    "- [Adaptive linear neurons and the convergence of learning](#Adaptive-linear-neurons-and-the-convergence-of-learning)\n",
    "    - [Minimizing cost functions with gradient descent](#Minimizing-cost-functions-with-gradient-descent)\n",
    "    - [Implementing an Adaptive Linear Neuron in Python](#Implementing-an-Adaptive-Linear-Neuron-in-Python)\n",
    "    - [Improving gradient descent through feature scaling](#Improving-gradient-descent-through-feature-scaling)\n",
    "    - [Large scale machine learning and stochastic gradient descent](#Large-scale-machine-learning-and-stochastic-gradient-descent)\n",
    "- [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial neurons - a brief glimpse into the early history of machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='./images/02_01.png', width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The formal definition of an artificial neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='./images/02_02.png', width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The perceptron learning rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='./images/02_03.png', width=600) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Image(filename='./images/02_04.png', width=600) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a perceptron learning algorithm in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An object-oriented perceptron API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Perceptron: #(object):\n",
    "    \"\"\"Perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "      Passes over the training dataset.\n",
    "    random_state : int\n",
    "      Random number generator seed for random weight\n",
    "      initialization.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "      Weights after fitting.\n",
    "    errors_ : list\n",
    "      Number of misclassifications (updates, errors) in each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, eta= 0.01, n_iter= 100, random_state= 1):\n",
    "        self.eta=          eta\n",
    "        self.n_iter=       n_iter\n",
    "        self.random_state= random_state\n",
    "\n",
    "    def fit(self, X, y, decisionRegionDraw= False):\n",
    "        \"\"\"Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "          Training vectors, where n_samples is the number of samples and\n",
    "          n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "          Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "        rgen= np.random.RandomState(self.random_state)\n",
    "        \n",
    "        self.w_     = rgen.normal(loc=   0.0, \n",
    "                                  scale= 0.01, \n",
    "                                  size=  1 + X.shape[1])\n",
    "        self.errors_= []\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, yi in zip(X, y):\n",
    "                \n",
    "                ### Rosenblatt's Perceptron Learning Rule:\n",
    "                \n",
    "                e=    yi - self.predict(xi) \n",
    "                \n",
    "                #self.w_[0]  +=  1 * e * self.eta\n",
    "                #self.w_[1:] += xi * e * self.eta\n",
    "                \n",
    "                x=          np.append([1], xi)\n",
    "                self.w_ +=  x * e * self.eta\n",
    "                              \n",
    "                errors += int(e != 0)\n",
    "            \n",
    "            self.errors_.append(errors)\n",
    "            \n",
    "            ### ry: 每一次 fit 畫一下 decision region\n",
    "            if decisionRegionDraw == True:\n",
    "                plot_decision_regions(X, y, classifier= self) #ppn)\n",
    "                plt.show()\n",
    "        ### ry\n",
    "        ### plt.show()\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, +1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.array([1, 2, 3])\n",
    "v2 = 0.5 * v1\n",
    "np.arccos(v1.dot(v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a perceptron model on the Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading-in the Iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n",
    "                 header= None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Note:\n",
    "\n",
    "\n",
    "You can find a copy of the Iris dataset (and all other datasets used in this book) in the code bundle of this book, which you can use if you are working offline or the UCI server at https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data is temporarily unavailable. For instance, to load the Iris dataset from a local directory, you can replace the line \n",
    "\n",
    "    df = pd.read_csv('https://archive.ics.uci.edu/ml/'\n",
    "        'machine-learning-databases/iris/iris.data', header=None)\n",
    " \n",
    "by\n",
    " \n",
    "    df = pd.read_csv('your/local/path/to/iris.data', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('iris.data', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[4], np.unique(df[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "Labels=   (-1, +1)\n",
    "Markers=  ('x',   'o')\n",
    "Colors=   ('blue','red')\n",
    "\n",
    "#\n",
    "# 數據預處理\n",
    "#\n",
    "# 從 df[4] (代表類別) 選 'Iris-setosa' 或 'Iris-versicolor'\n",
    "# df ==> df2\n",
    "#\n",
    "df2= df[(df[4] == 'Iris-setosa') | \n",
    "        (df[4] == 'Iris-versicolor')]\n",
    "\n",
    "print('df2= ', df2)\n",
    "print('df2.describe()= ', df2.describe())\n",
    "\n",
    "#\n",
    "# 抓出特徵\n",
    "# extract sepalLength@Column[0] and petalLength@Column[2]\n",
    "X= df2[[0,2]].values\n",
    "#print('X= ',X)\n",
    "\n",
    "# 抓出類別\n",
    "y= df2[4].values\n",
    "#print('y= ',y)\n",
    "\n",
    "# 文字類別 ('Iris-setosa', 非 'Iris-setosa')\n",
    "# ==> 數字類別 (+1, -1)\n",
    "y= np.where(y == 'Iris-setosa', # if\n",
    "            +1,                 # then\n",
    "            -1)                 # else\n",
    "#print('y= ',y)\n",
    "\n",
    "uniqueY= np.unique(y)\n",
    "#print('uniqueY= ', uniqueY)\n",
    "\n",
    "#\n",
    "# 數據做圖\n",
    "#\n",
    "\n",
    "# 陽春版\n",
    "x0= X[:, 0], \n",
    "x1= X[:, 1]\n",
    "plt.scatter(x0, x1)\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "plt.show()\n",
    "\n",
    "# 精細版\n",
    "for i, b in enumerate(Labels): \n",
    "    # b= -1, +1\n",
    "    x0= X[y == b, 0] \n",
    "    x1= X[y == b, 1]\n",
    "\n",
    "    plt.scatter(x= x0, \n",
    "                y= x1, \n",
    "                label= b,\n",
    "                c=      Colors[i],\n",
    "                marker= Markers[i], \n",
    "                edgecolor='black')\n",
    "\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "N= 1 # DownSample Factor\n",
    "\n",
    "# select setosa and versicolor\n",
    "y= df.iloc[0:100:N, 4].values\n",
    "\n",
    "# 'Iris-setosa' ==> '+1'\n",
    "# else ==> '-1'\n",
    "y= np.where(y == 'Iris-setosa', +1, -1)\n",
    "\n",
    "# extract sepalLength@Column[0] and petalLength@Column[2]\n",
    "X= df.iloc[0:100:N, [0,2]].values\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[0:50//N, 0], \n",
    "            X[0:50//N, 1],\n",
    "            color='red', \n",
    "            marker='o', \n",
    "            label='setosa, (+1)')\n",
    "\n",
    "plt.scatter(X[50//N:100//N, 0], \n",
    "            X[50//N:100//N, 1],\n",
    "            color='blue', \n",
    "            marker='x', \n",
    "            label='versicolor, (-1)')\n",
    "\n",
    "plt.xlabel('sepalLen [cm]')\n",
    "plt.ylabel('petalLen [cm]')\n",
    "plt.legend()#loc= 'upper left')\n",
    "\n",
    "# plt.savefig('images/02_06.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the perceptron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppn= Perceptron(eta= 0.1, n_iter= 10)\n",
    "\n",
    "ppn.fit(X, y, decisionRegionDraw= True)\n",
    "\n",
    "plt.plot(range(1, len(ppn.errors_) + 1), \n",
    "         ppn.errors_, \n",
    "         marker='o')\n",
    "plt.xlabel('Epochs (時代，輪)')\n",
    "plt.ylabel('Errors (錯誤數)')\n",
    "\n",
    "# plt.savefig('images/02_07.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Ex000: 開始做辨認 ..., 並檢查、分析結果\n",
    "#\n",
    "yHat= ppn.predict(X)\n",
    "\n",
    "e= y-yHat\n",
    "df000= pd.DataFrame(list(zip(X,yHat,y,e)),columns= ('x','yHat','y','e'))\n",
    "\n",
    "df000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function for plotting decision regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "markers=  ('x',   'o')\n",
    "colors=   ('blue','red')\n",
    "colormap= ListedColormap(colors)\n",
    "    \n",
    "def plot_decision_regions(X, y, classifier, resolution= .01, regionBoundary= 'contourf'):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "     \n",
    "    # 以本題而言，  '-1', '+1' ==> index= [0, 1] ==> ['x', 'o']\n",
    "    uniqueY= np.unique(y) # == [-1, +1]\n",
    "    # np.unique(y) Returns the sorted unique elements of an array.\n",
    "    \n",
    "    # plot the decision surface\n",
    "    x0_min, x0_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x1_min, x1_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    #製造所有範圍內的「網格點」\n",
    "    xx0, xx1= np.meshgrid(np.arange(x0_min, x0_max, resolution),\n",
    "                          np.arange(x1_min, x1_max, resolution))\n",
    "    \n",
    "    # 全面對所有網格點做 predict (工程浩大喔！) \n",
    "    Z= classifier.predict(np.array([xx0.ravel(), xx1.ravel()]).T)\n",
    "    \n",
    "    Z= Z.reshape(xx0.shape)\n",
    "    \n",
    "    if regionBoundary == 'contourf':\n",
    "        plt.contourf(xx0, xx1, Z, alpha= .1, cmap= colormap)\n",
    "    else: # 'pcolormesh'\n",
    "        plt.pcolormesh(xx0, xx1, Z, alpha= .1, cmap= colormap)\n",
    "    \n",
    "    plt.xlim(xx0.min(), xx0.max())\n",
    "    plt.ylim(xx1.min(), xx1.max())\n",
    "\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(uniqueY):\n",
    "        \n",
    "        x0= X[y == cl, 0] \n",
    "        x1= X[y == cl, 1]\n",
    "            \n",
    "        plt.scatter(x= x0, \n",
    "                    y= x1, \n",
    "                    label= cl,\n",
    "                    alpha= .9, \n",
    "                    c=      colors[idx],\n",
    "                    marker= markers[idx], \n",
    "                    edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X, y, classifier= ppn)\n",
    "\n",
    "plt.xlabel('sepalLen [cm]')\n",
    "plt.ylabel('petalLen [cm]')\n",
    "plt.legend() #loc= 'upper left')\n",
    "\n",
    "\n",
    "# plt.savefig('images/02_08.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive linear neurons and the convergence of learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing cost functions with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Image(filename='./images/02_09.png', width=600) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Image(filename='./images/02_10.png', width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing an adaptive linear neuron in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalineGD(object):\n",
    "    \"\"\"ADAptive LInear NEuron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "      Passes over the training dataset.\n",
    "    random_state : int\n",
    "      Random number generator seed for random weight\n",
    "      initialization.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "      Weights after fitting.\n",
    "    cost_ : list\n",
    "      Sum-of-squares cost function value in each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, eta= 0.01, n_iter= 50, random_state= 1):\n",
    "        self.eta=          eta\n",
    "        self.n_iter=       n_iter\n",
    "        self.random_state= random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "          Training vectors, where n_samples is the number of samples and\n",
    "          n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "          Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        #\n",
    "        # RandomState() exposes a number of methods for generating random numbers\n",
    "        #\n",
    "        # A fixed seed and a fixed series of calls to ‘RandomState’ methods \n",
    "        # using the same parameters will always produce the same results\n",
    "        #\n",
    "        #\n",
    "        \n",
    "        self.w_ =    rgen.normal(loc= 0.0, scale= 0.01, size= 1 + X.shape[1])\n",
    "        self.cost_ = []\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            # Please note that the \"activation\" method has no effect\n",
    "            # in the code since it is simply an identity function. We\n",
    "            # could write `output = self.net_input(X)` directly instead.\n",
    "            # The purpose of the activation is more conceptual, i.e.,  \n",
    "            # in the case of logistic regression (as we will see later), \n",
    "            # we could change it to\n",
    "            # a sigmoid function to implement a logistic regression classifier.\n",
    "            output = self.activation(net_input)\n",
    "            errors = (y - output)\n",
    "            self.w_[1:] += self.eta * X.T.dot(errors)\n",
    "            self.w_[0] += self.eta * errors.sum()\n",
    "            cost = (errors**2).sum() / 2.0\n",
    "            self.cost_.append(cost)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"Compute linear activation, just pass it through\"\"\"\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "ada1= AdalineGD(n_iter= 10, eta= 0.01).fit(X, y)\n",
    "\n",
    "ax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker='o')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('log(Sum-squared-error)')\n",
    "ax[0].set_title('Adaline - Learning rate 0.01')\n",
    "\n",
    "ada2= AdalineGD(n_iter= 10, eta= 0.0001).fit(X, y)\n",
    "\n",
    "ax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker='o')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Sum-squared-error')\n",
    "ax[1].set_title('Adaline - Learning rate 0.0001')\n",
    "\n",
    "# plt.savefig('images/02_11.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X, y, classifier=ada1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X, y, classifier=ada2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='./images/02_12.png', width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving gradient descent through feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='./images/02_13.png', width=700) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize features\n",
    "X_std = np.copy(X)\n",
    "X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()\n",
    "X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdalineGD(n_iter=15, eta=0.01)\n",
    "ada.fit(X_std, y)\n",
    "\n",
    "plot_decision_regions(X_std, y, classifier=ada)\n",
    "plt.title('Adaline - Gradient Descent')\n",
    "plt.xlabel('sepal length [standardized]')\n",
    "plt.ylabel('petal length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/02_14_1.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Sum-squared-error')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/02_14_2.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large scale machine learning and stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalineSGD(object):\n",
    "    \"\"\"ADAptive LInear NEuron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "      Passes over the training dataset.\n",
    "    shuffle : bool (default: True)\n",
    "      Shuffles training data every epoch if True to prevent cycles.\n",
    "    random_state : int\n",
    "      Random number generator seed for random weight\n",
    "      initialization.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "      Weights after fitting.\n",
    "    cost_ : list\n",
    "      Sum-of-squares cost function value averaged over all\n",
    "      training samples in each epoch.\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.w_initialized = False\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "          Training vectors, where n_samples is the number of samples and\n",
    "          n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "          Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "        self._initialize_weights(X.shape[1])\n",
    "        self.cost_ = []\n",
    "        for i in range(self.n_iter):\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "            cost = []\n",
    "            for xi, target in zip(X, y):\n",
    "                cost.append(self._update_weights(xi, target))\n",
    "            avg_cost = sum(cost) / len(y)\n",
    "            self.cost_.append(avg_cost)\n",
    "        return self\n",
    "\n",
    "    def partial_fit(self, X, y):\n",
    "        \"\"\"Fit training data without reinitializing the weights\"\"\"\n",
    "        if not self.w_initialized:\n",
    "            self._initialize_weights(X.shape[1])\n",
    "        if y.ravel().shape[0] > 1:\n",
    "            for xi, target in zip(X, y):\n",
    "                self._update_weights(xi, target)\n",
    "        else:\n",
    "            self._update_weights(X, y)\n",
    "        return self\n",
    "\n",
    "    def _shuffle(self, X, y):\n",
    "        \"\"\"Shuffle training data\"\"\"\n",
    "        r = self.rgen.permutation(len(y))\n",
    "        return X[r], y[r]\n",
    "    \n",
    "    def _initialize_weights(self, m):\n",
    "        \"\"\"Initialize weights to small random numbers\"\"\"\n",
    "        self.rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=1 + m)\n",
    "        self.w_initialized = True\n",
    "        \n",
    "    def _update_weights(self, xi, target):\n",
    "        \"\"\"Apply Adaline learning rule to update the weights\"\"\"\n",
    "        output = self.activation(self.net_input(xi))\n",
    "        error = (target - output)\n",
    "        self.w_[1:] += self.eta * xi.dot(error)\n",
    "        self.w_[0] += self.eta * error\n",
    "        cost = 0.5 * error**2\n",
    "        return cost\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"Compute linear activation\"\"\"\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdalineSGD(n_iter=15, eta=0.01, random_state=1)\n",
    "ada.fit(X_std, y)\n",
    "\n",
    "plot_decision_regions(X_std, y, classifier=ada)\n",
    "plt.title('Adaline - Stochastic Gradient Descent')\n",
    "plt.xlabel('sepal length [standardized]')\n",
    "plt.ylabel('petal length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/02_15_1.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Cost')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/02_15_2.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada.partial_fit(X_std[0, :], y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "Readers may ignore the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../.convert_notebook_to_script.py --input ch02.ipynb --output ch02.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
